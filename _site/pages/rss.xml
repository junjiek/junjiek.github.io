<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>柯均洁的博客</title>
    <link>http://junjiek.com</link>
    <description>柯均洁的博客</description>
    
      <item>
        <title>Adaboost/Multiboost/Liblinear Classification Experiment Results</title>
        <link>http://junjiek.com/2015/07/12/bootsing-svm-insights.html</link>
        <guid isPermaLink="true">http://junjiek.com/2015/07/12/bootsing-svm-insights.html</guid>
        <pubDate>Sun, 12 Jul 2015 00:00:00 -0400</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Test and compare the existing svm and boosting packages from two perspectives:&lt;br /&gt;
1. Running time and accuracy with different size of training set&lt;br /&gt;
2. Running time and accuracy with different size of feature&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;different-sample-size&quot;&gt;1. Different sample size&lt;/h2&gt;

&lt;h3 id=&quot;descriptions&quot;&gt;1.1 Descriptions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset: &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#rcv1.binary&quot;&gt;rcv1.binary&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Training set: Randomly sample 0.1, 0.2, …, 1 among &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/rcv1_train.binary.bz2&quot;&gt;rcv1_train.binary&lt;/a&gt; to be the training set&lt;/li&gt;
      &lt;li&gt;Test set: Fixed subset (0.1 random sample) of &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/rcv1_test.binary.bz2&quot;&gt;rcv1_test.binary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Measurement:
    &lt;ul&gt;
      &lt;li&gt;Prediction accuracy on test set&lt;/li&gt;
      &lt;li&gt;Training time&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Parameters:
    &lt;ul&gt;
      &lt;li&gt;Liblinear: Use the -C option to find the best c using cross-validation, use the best c to train the model and report the training time and accuracy on the test set.&lt;/li&gt;
      &lt;li&gt;Multiboost:
        &lt;ul&gt;
          &lt;li&gt;BaseLearner: SingleSparseStumpLearner (much faster than the SingleStumpLearner)&lt;/li&gt;
          &lt;li&gt;Rounds: 50&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Adaboost:
        &lt;ul&gt;
          &lt;li&gt;Type: gentle&lt;/li&gt;
          &lt;li&gt;Rounds: 50&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;1.2 Results&lt;/h3&gt;

&lt;h5&gt;All&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;http://7xk717.com1.z0.glb.clouddn.com/samplesize_lib_ada_mul.png&quot; alt=&quot;samplesize_all&quot; /&gt;&lt;/p&gt;

&lt;h5&gt;Liblinear&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;http://7xk717.com1.z0.glb.clouddn.com/samplesize_liblinear.png&quot; alt=&quot;samplesize_liblinear&quot; /&gt;&lt;/p&gt;

&lt;h5&gt;Adaboost&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;http://7xk717.com1.z0.glb.clouddn.com/samplesize_adaboost.png&quot; alt=&quot;samplesize_adaboost&quot; /&gt;&lt;/p&gt;

&lt;h5&gt;Multiboost&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;http://7xk717.com1.z0.glb.clouddn.com/samplesize_multiboost.png&quot; alt=&quot;samplesize_multiboost&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusions-and-analysis&quot;&gt;1.3 Conclusions and Analysis&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Liblinear performs the best with the highest accuracy rate and minimum running time.&lt;/li&gt;
  &lt;li&gt;For all 3 algorithms, the testing accuracy is converging and the running time increases linearly with the growth of training sample size.&lt;/li&gt;
  &lt;li&gt;For those two boosting algorithms
    &lt;ul&gt;
      &lt;li&gt;Adaboost performs better in prediction accuracy.&lt;/li&gt;
      &lt;li&gt;Multiboost takes less time to train, but it also seems to take more time to run the test especially when the test set is large. Though I did not record the testing time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;different-feature-size&quot;&gt;2. Different feature size&lt;/h2&gt;

&lt;h3 id=&quot;descriptions-1&quot;&gt;2.1 Descriptions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset: &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#rcv1.binary&quot;&gt;rcv1.binary&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Traing set:
        &lt;ul&gt;
          &lt;li&gt;Randomly sample 0.3 among &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/rcv1_train.binary.bz2&quot;&gt;rcv1_train.binary&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;For each feature, calculate the p-value of the two-sample t-test between the positive and negative samples.&lt;/li&gt;
          &lt;li&gt;Sort features by desendant p-value&lt;/li&gt;
          &lt;li&gt;Select the first 0.1, 0.2, …, 1 features to generate new training data&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Test set:
        &lt;ul&gt;
          &lt;li&gt;Use one fixed test set: 0.1 sample from &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/rcv1_test.binary.bz2&quot;&gt;rcv1_test.binary&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;For each new training data, choose same features of the test set to generate new test data.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Measurement:
    &lt;ul&gt;
      &lt;li&gt;Prediction accuracy on test set&lt;/li&gt;
      &lt;li&gt;Running time&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Parameters:
    &lt;ul&gt;
      &lt;li&gt;Same as previous section&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results-1&quot;&gt;2.2 Results&lt;/h3&gt;

&lt;h5&gt;All&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;http://7xk717.com1.z0.glb.clouddn.com/featuresize_lib_ada_mul.png&quot; alt=&quot;featuresize_all&quot; /&gt;&lt;/p&gt;

&lt;h5&gt;Liblinear&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;http://7xk717.com1.z0.glb.clouddn.com/featuresize_liblinear.png&quot; alt=&quot;featuresize_liblinear&quot; /&gt;&lt;/p&gt;

&lt;h5&gt;Adaboost&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;http://7xk717.com1.z0.glb.clouddn.com/featuresize_adaboost.png&quot; alt=&quot;featuresize_adaboost&quot; /&gt;&lt;/p&gt;

&lt;h5&gt;Multiboost&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;http://7xk717.com1.z0.glb.clouddn.com/featuresize_multiboost.png&quot; alt=&quot;featuresize_multiboost&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusions-and-analysis-1&quot;&gt;2.3 Conclusions and Analysis&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Liblinear performs the best with the highest accuracy rate and minimum running time. Its testing accuracy is converging. Its running time is converging intially, but when feature size reaches &lt;strong&gt;26k&lt;/strong&gt; it suddenly jumps up and then decreses gradually.&lt;/li&gt;
  &lt;li&gt;For those two boosting algorithms
    &lt;ul&gt;
      &lt;li&gt;Running time grows linearly&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Testing accuracy behaves like a step curve&lt;/strong&gt;, the accuracy remains almost &lt;strong&gt;unchanged&lt;/strong&gt; for both algorithms initially, but when feature size reaches &lt;strong&gt;26k&lt;/strong&gt;, it jumps up suddenly.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;It seems that interesting things have happened when feature size reaches 26k. Further examinations and experiments on other datasets are needed to find out the reason.&lt;/li&gt;
&lt;/ul&gt;
</description>
      </item>
    
      <item>
        <title>ssh使用方法总结</title>
        <link>http://junjiek.com/2015/07/07/ssh.html</link>
        <guid isPermaLink="true">http://junjiek.com/2015/07/07/ssh.html</guid>
        <pubDate>Tue, 07 Jul 2015 00:00:00 -0400</pubDate>
        <description>&lt;h2 id=&quot;section&quot;&gt;常用的命令&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;scp：可以在本地和远程传输文件&lt;/li&gt;
  &lt;li&gt;wget：直接通过链接下载文件&lt;/li&gt;
  &lt;li&gt;htop：查看任务运行情况&lt;/li&gt;
  &lt;li&gt;tmux：在tmux里面跑程序不用担心ssh连接
    &lt;ul&gt;
      &lt;li&gt;tmux attach：重新回到界面&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;nohup：让程序在关闭窗口（切换SSH连接）的时候程序还能继续在后台运行。&lt;br /&gt;
 nohup Command [Arg…] [&amp;amp;]&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;ssh&quot;&gt;ssh自动登录（无需密码）&lt;/h2&gt;

&lt;p&gt;生成密钥对&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh-keygen -t rsa
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后改一下 .ssh 目录的权限&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$chmod 755 ~/.ssh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把这个密钥对中的公共密钥复制到你要访问的机器上去，并保存为 ~/.ssh/authorized_keys&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scp ~/.ssh/id_rsa.pub usrname@host:~/.ssh/authorized_keys
&lt;/code&gt;&lt;/pre&gt;

</description>
      </item>
    
      <item>
        <title>Boosting及SVM的工具使用总结</title>
        <link>http://junjiek.com/2015/07/06/boosting-and-svm-tools.html</link>
        <guid isPermaLink="true">http://junjiek.com/2015/07/06/boosting-and-svm-tools.html</guid>
        <pubDate>Mon, 06 Jul 2015 00:00:00 -0400</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Wanna compare the performance of ensemble methods with svm, here’s a summary of the usage of some existing tools.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;document-datasets&quot;&gt;Document datasets&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/&quot;&gt;Libsvm Datasets&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#news20&quot;&gt;news20&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#news20.binary&quot;&gt;news20.binary&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#rcv1.binary&quot;&gt;rcv1.binary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;boosting&quot;&gt;Boosting&lt;/h1&gt;

&lt;h2 id=&quot;multiboosthttpwwwmultiboostorg&quot;&gt;&lt;a href=&quot;http://www.multiboost.org/&quot;&gt;Multiboost&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;资源：&lt;a href=&quot;http://www.multiboost.org/download/MultiBoost-1.2.02.zip?attredirects=0&amp;amp;d=1&quot;&gt;下载链接&lt;/a&gt;、&lt;a href=&quot;http://docs.google.com/viewer?a=v&amp;amp;pid=sites&amp;amp;srcid=bXVsdGlib29zdC5vcmd8d3d3fGd4OjE5YjY0MzgwNjNlNjU1NmY&quot;&gt;使用文档&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;使用方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./multiboost --stronglearner [AdaBoostMH/ArcGV/FilterBoost/VJcascade/SoftCascade]
             --fileformat [arff/svmlight/simple]
             --traintest &amp;lt;trainfile&amp;gt; &amp;lt;testfile&amp;gt; &amp;lt;iteration_num&amp;gt;
             --verbose [0-5]
             --learnertype SingleStumpLearner
             --outputfile &amp;lt;results.dta&amp;gt;
             --shypname shyp.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;adaboosthttpsgithubcomyamaguchi23adaboost&quot;&gt;&lt;a href=&quot;https://github.com/yamaguchi23/adaboost&quot;&gt;Adaboost&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;使用方法：&lt;/p&gt;
&lt;h5&gt;Training&lt;/h5&gt;

&lt;pre&gt;&lt;code&gt;$ ./abtrain [options] training_set_file [model_file]  
  options:  
    -t: type of boosting (0:discrete, 1:real, 2:gentle) [default:2]  
    -r: the number of rounds [default:100]  
    -v: verbose&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h5&gt;Prediction&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;$ ./abpredict [options] test_set_file model_file  
  options:  
    -o: output score file  
    -v: verbose&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;svm&quot;&gt;SVM&lt;/h1&gt;

&lt;p&gt;Documentation：（&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf&quot;&gt;Libsvm&lt;/a&gt; &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf&quot;&gt;Liblinear&lt;/a&gt;）&lt;/p&gt;

&lt;h2 id=&quot;libsvmhttpwwwcsientuedutwcjlinlibsvmindexhtml&quot;&gt;&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html&quot;&gt;Libsvm&lt;/a&gt;&lt;/h2&gt;

&lt;h4&gt;Procedure&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;SVM formatting&lt;/li&gt;
  &lt;li&gt;Scaling ( linearly to &lt;script type=&quot;math/tex&quot;&gt;[−1, +1]&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;[0, 1]&lt;/script&gt; )&lt;/li&gt;
  &lt;li&gt;Try the RBF kernel&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x, y)=e^{-\gamma ||x-y||^2}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Use cross-validation to find the best parameter &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;Try different pairs of &lt;script type=&quot;math/tex&quot;&gt;(C, \gamma)&lt;/script&gt; (grid-search)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Use the best parameter &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; to train the whole training set&lt;/li&gt;
  &lt;li&gt;Test&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Example&lt;/h4&gt;
&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Scaled sets with default parameters&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;$ ./svm-scale -l -1 -u 1 -s range3 svmguide3 &amp;gt; svmguide3.scale
$ ./svm-scale -r range3 svmguide3.t &amp;gt; svmguide3.t.scale
$ ./svm-train svmguide3.scale
$ ./svm-predict svmguide3.t.scale svmguide3.scale.model svmguide3.t.predict
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;Scaled sets with parameter selection&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;$ python grid.py svmguide3.scale
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;Using an automatic scrip&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;$ python easy.py svmguide3 svmguide3.t
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;liblinearhttpwwwcsientuedutwcjlinliblinear&quot;&gt;&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;Liblinear&lt;/a&gt;&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Particularly useful for document data. (“bag-of-words” model)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;The -C option efficiently conducts cross validation several times and finds&lt;br /&gt;
the best parameter automatically.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;$ ./train -C -s 2 news20.scale
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;Train and predict&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;$ ./train -c 1 -s 2 trainfile modelfile
$ ./predict testfile modelfile prediction
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;The -q option disable verbose&lt;/p&gt;
&lt;/blockquote&gt;

</description>
      </item>
    
  </channel>
</rss>